# 第三周作业题

```py
selected_words = ['awesome', 'great', 'fantastic', 'amazing', 'love', 'horrible', 'bad', 'terrible', 'awful', 'wow', 'hate']
```

1. 从 selected_words 中，哪个在数据集中最常用？

方法：对 selected_words 的 12 个单词都应用函数创造新列：

```py
products['word_count'] = graphlab.text_analytics.count_words(products['review'])

def awesome_count(dict):
    if 'awesome' in dict:
        return dict['awesome']
    else:
        return 0
        

products['awesome'] = products['word_count'].apply(awesome_count)

for word in selected_words:
    print word
    print products[word].sum()
```

结果

```
awesome
2090
great
45206
fantastic
932
amazing
1363
love
42065
horrible
734
bad
3724
terrible
748
awful
383
wow
144
hate
1220
```

答案：great

2. 训练好模型后，找到各特征值的权重。

```py
products = products[products['rating'] != 3]
products['sentiment'] = products['rating'] >= 4

train_data,test_data = products.random_split(.8, seed=0)

selected_words_model = graphlab.logistic_classifier.create(train_data, target='sentiment', features=selected_words, validation_set=test_data)

selected_words_model['coefficients']
```

结果：

```
name 	index 	class 	value 	stderr
(intercept) 	None 	1 	1.36728315229 	0.00861805467824
awesome 	None 	1 	1.05800888878 	0.110865296265
great 	None 	1 	0.883937894898 	0.0217379527921
fantastic 	None 	1 	0.891303090304 	0.154532343591
amazing 	None 	1 	0.892802422508 	0.127989503231
love 	None 	1 	1.39989834302 	0.0287147460124
horrible 	None 	1 	-1.99651800559 	0.0973584169028
bad 	None 	1 	-0.985827369929 	0.0433603009142
terrible 	None 	1 	-2.09049998487 	0.0967241912229
awful 	None 	1 	-1.76469955631 	0.134679803365
wow 	None 	1 	-0.0541450123333 	0.275616449416
hate 	None 	1 	-1.40916406276 	0.0771983993506
```

答案：

3. 用测试集测试：

```py
selected_words_model.evaluate(test_data)
```

```
{'accuracy': 0.8431419649291376,
 'auc': 0.6648096413721418,
 'confusion_matrix': Columns:
 	target_label	int
 	predicted_label	int
 	count	int
 
 Rows: 4
 
 Data:
 +--------------+-----------------+-------+
 | target_label | predicted_label | count |
 +--------------+-----------------+-------+
 |      0       |        0        |  234  |
 |      0       |        1        |  5094 |
 |      1       |        1        | 27846 |
 |      1       |        0        |  130  |
 +--------------+-----------------+-------+
 [4 rows x 3 columns],
 'f1_score': 0.914242563530107,
 'log_loss': 0.4054747110365649,
 'precision': 0.8453551912568306,
 'recall': 0.9953531598513011,
 'roc_curve': Columns:
 	threshold	float
 	fpr	float
 	tpr	float
 	p	int
 	n	int
 
 Rows: 100001
 
 Data:
 +-----------+-----+-----+-------+------+
 | threshold | fpr | tpr |   p   |  n   |
 +-----------+-----+-----+-------+------+
 |    0.0    | 1.0 | 1.0 | 27976 | 5328 |
 |   1e-05   | 1.0 | 1.0 | 27976 | 5328 |
 |   2e-05   | 1.0 | 1.0 | 27976 | 5328 |
 |   3e-05   | 1.0 | 1.0 | 27976 | 5328 |
 |   4e-05   | 1.0 | 1.0 | 27976 | 5328 |
 |   5e-05   | 1.0 | 1.0 | 27976 | 5328 |
 |   6e-05   | 1.0 | 1.0 | 27976 | 5328 |
 |   7e-05   | 1.0 | 1.0 | 27976 | 5328 |
 |   8e-05   | 1.0 | 1.0 | 27976 | 5328 |
 |   9e-05   | 1.0 | 1.0 | 27976 | 5328 |
 +-----------+-----+-----+-------+------+
 [100001 rows x 5 columns]
 Note: Only the head of the SFrame is printed.
 You can use print_rows(num_rows=m, num_columns=n) to print more rows and columns.}
```

4. 解释模型之间的性能差异：

```py
diaper_champ_reviews = products[products['name'] == 'Baby Trend Diaper Champ']

diaper_champ_reviews['predicted_sentiment'] = selected_words_model.predict(diaper_champ_reviews, output_type='probability')
```

```py
sentiment_model.predict(diaper_champ_reviews[0:1], output_type='probability')
```

```
dtype: float
Rows: 1
[0.9584435808934196]
```

```py
selected_words_model.predict(diaper_champ_reviews[0:1], output_type='probability')
```

```
dtype: float
Rows: 1
[0.7969408512906712]
```