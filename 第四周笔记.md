# 第四周笔记

文本分析 & 聚类

## 词袋（Bags of words）模型

忽略了词的次序，只考虑词的频次。稀疏向量表示。

将两篇文章得到的向量点乘，得到一个相似度。

单纯的词袋模型倾向于长文章，因为词频乘2，得到的相似度乘4。可以通过归一化向量解决

## 关键词

### 生僻词和常用词

生僻词：在语料库（corpus）中出现较少，描述一篇文章的独特之处。

要设法增加生僻词的权重。对每个单词，根据其出现在语料库中的文档数，减少其权重。

我们不想强调常用词（the、a、it），想强调生僻词，也想强调关键词（跟我们读的文档相关的单词）。

局部常见（common locally）：在文档中出现频率较高
全局罕见（rare globally）：在语料库中相对少见

描述所谓关键词的特征，就是在某种局部常见率和全局罕见率中进行权衡。一种权衡方式就是 TF-IDF（词频-逆向文件频率法，Term frequency - inverse document frequency）。

### TF-IDF（词频-逆向文件频率法）

tf-idf = #docs / (1 + #docs using word)

## 最邻近域搜索（Nearest neighbor search）和 k-近邻搜索

设立近似度阈值，在语料库中查找可以推荐的相似文章



## 我的问题

tf-idf 怎么转换为特征

## 代码分析

`stack()`函数将 SFrame 里字典的一列取出，然后和另外几列放在一起。

```py
obama_word_count_table = obama[['word_count']].stack('word_count', new_column_name = ['word', 'count'])
```

